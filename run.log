[2022-06-22 16:17:45,457][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp6yojwmii
[2022-06-22 16:17:45,457][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp6yojwmii/_remote_module_non_sriptable.py
[2022-06-22 16:17:46,811][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-22 16:17:46,815][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-22 16:17:46,827][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-22 16:17:46,827][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-22 18:58:19,351][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpx557xkub
[2022-06-22 18:58:19,351][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpx557xkub/_remote_module_non_sriptable.py
[2022-06-22 19:01:40,510][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-22 19:01:40,514][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-22 19:01:40,525][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-22 19:01:40,526][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-22 19:10:57,823][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp9gbvatd9
[2022-06-22 19:10:57,824][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp9gbvatd9/_remote_module_non_sriptable.py
[2022-06-22 19:11:40,204][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-22 19:11:40,209][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-22 19:11:40,224][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-22 19:11:40,224][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 13:54:23,308][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpcpv8nupt
[2022-06-23 13:54:23,309][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpcpv8nupt/_remote_module_non_sriptable.py
[2022-06-23 13:54:38,369][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 13:54:38,373][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 13:54:38,384][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 13:54:38,384][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 14:15:39,543][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp5s3tzl_d
[2022-06-23 14:15:39,543][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp5s3tzl_d/_remote_module_non_sriptable.py
[2022-06-23 14:15:40,636][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 14:15:40,639][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 14:15:40,651][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 14:15:40,651][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 17:58:57,593][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpnhaign8a
[2022-06-23 17:58:57,594][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpnhaign8a/_remote_module_non_sriptable.py
[2022-06-23 17:58:58,746][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 17:58:58,749][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 17:58:58,760][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 17:58:58,760][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 18:01:08,409][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpe7uj4hj0
[2022-06-23 18:01:08,410][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpe7uj4hj0/_remote_module_non_sriptable.py
[2022-06-23 18:01:09,429][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 18:01:09,434][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 18:01:09,444][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 18:01:09,444][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 18:19:18,238][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp71hnwpr2
[2022-06-23 18:19:18,239][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp71hnwpr2/_remote_module_non_sriptable.py
[2022-06-23 18:19:19,245][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 18:19:19,249][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 18:19:19,261][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 18:19:19,262][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 18:24:14,928][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpkcncxz12
[2022-06-23 18:24:14,929][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpkcncxz12/_remote_module_non_sriptable.py
[2022-06-23 18:24:16,075][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 18:24:16,079][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 18:24:16,091][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 18:24:16,092][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 18:28:43,508][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp9r5s3lha
[2022-06-23 18:28:43,509][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp9r5s3lha/_remote_module_non_sriptable.py
[2022-06-23 18:28:44,449][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 18:28:44,453][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 18:28:44,464][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 18:28:44,464][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 18:33:00,031][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp13iaeblb
[2022-06-23 18:33:00,031][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp13iaeblb/_remote_module_non_sriptable.py
[2022-06-23 18:33:01,086][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 18:33:01,091][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 18:33:01,104][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 18:33:01,104][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 18:34:42,244][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpnrzvyt13
[2022-06-23 18:34:42,245][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpnrzvyt13/_remote_module_non_sriptable.py
[2022-06-23 18:34:43,260][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 18:34:43,264][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 18:34:43,275][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 18:34:43,275][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 18:34:59,907][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-23 18:35:18,592][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-24 17:10:28,708][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpbs79_uar
[2022-06-24 17:10:28,709][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpbs79_uar/_remote_module_non_sriptable.py
[2022-06-24 17:10:29,609][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 17:10:29,611][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 17:10:29,617][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 17:10:29,617][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 17:10:56,178][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-06-24 17:13:29,781][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpqprxxn0c
[2022-06-24 17:13:29,781][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpqprxxn0c/_remote_module_non_sriptable.py
[2022-06-24 17:13:30,986][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 17:13:30,988][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 17:13:30,996][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 17:13:30,996][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 17:14:59,650][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpxcnyjj4u
[2022-06-24 17:14:59,650][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpxcnyjj4u/_remote_module_non_sriptable.py
[2022-06-24 17:15:00,564][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 17:15:00,566][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 17:15:00,574][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 17:15:00,575][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 17:16:57,002][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpzlf4l20d
[2022-06-24 17:16:57,002][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpzlf4l20d/_remote_module_non_sriptable.py
[2022-06-24 17:16:58,009][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 17:16:58,010][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 17:16:58,015][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 17:16:58,015][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 17:17:06,525][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-24 17:17:08,363][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:146: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...
  "`training_step` returned `None`. If this was on purpose, ignore this warning..."

[2022-06-24 17:19:43,645][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-06-24 17:34:50,203][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp_kbcojcr
[2022-06-24 17:34:50,204][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp_kbcojcr/_remote_module_non_sriptable.py
[2022-06-24 17:34:51,153][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 17:34:51,156][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 17:34:51,165][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 17:34:51,165][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 17:34:59,205][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-24 17:35:01,862][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:146: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...
  "`training_step` returned `None`. If this was on purpose, ignore this warning..."

[2022-06-24 17:35:33,491][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-06-24 17:39:02,328][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpugw5d1kh
[2022-06-24 17:39:02,329][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpugw5d1kh/_remote_module_non_sriptable.py
[2022-06-24 17:39:03,245][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 17:39:03,249][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 17:39:03,260][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 17:39:03,261][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 17:39:11,871][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (5) is smaller than the logging interval Trainer(log_every_n_steps=200). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  f"The number of training samples ({self.num_training_batches}) is smaller than the logging interval"

[2022-06-24 18:02:42,618][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpo0mqmby7
[2022-06-24 18:02:42,620][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpo0mqmby7/_remote_module_non_sriptable.py
[2022-06-24 18:02:43,520][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 18:02:43,524][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 18:02:43,535][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 18:02:43,536][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 18:02:52,692][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (5) is smaller than the logging interval Trainer(log_every_n_steps=200). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  f"The number of training samples ({self.num_training_batches}) is smaller than the logging interval"

[2022-06-24 18:09:51,166][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpuk27vrih
[2022-06-24 18:09:51,167][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpuk27vrih/_remote_module_non_sriptable.py
[2022-06-24 18:09:52,151][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 18:09:52,155][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 18:09:52,166][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 18:09:52,166][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 18:11:10,982][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp4k4rvqt9
[2022-06-24 18:11:10,982][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp4k4rvqt9/_remote_module_non_sriptable.py
[2022-06-24 18:11:11,924][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 18:11:11,927][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 18:11:11,938][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 18:11:11,938][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 18:19:25,134][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpfena6eby
[2022-06-24 18:19:25,135][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpfena6eby/_remote_module_non_sriptable.py
[2022-06-24 18:19:26,381][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 18:19:26,382][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 18:19:26,387][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 18:19:26,387][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 18:19:39,213][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-24 18:19:44,290][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:146: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...
  "`training_step` returned `None`. If this was on purpose, ignore this warning..."

[2022-06-24 18:22:15,917][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-06-24 18:25:29,887][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpewivif0o
[2022-06-24 18:25:29,887][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpewivif0o/_remote_module_non_sriptable.py
[2022-06-24 18:25:31,025][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 18:25:31,029][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 18:25:31,040][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 18:25:31,040][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 18:25:54,191][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-24 18:26:11,283][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-24 18:29:29,412][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp1yse3wr4
[2022-06-24 18:29:29,412][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp1yse3wr4/_remote_module_non_sriptable.py
[2022-06-24 18:29:30,441][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 18:29:30,445][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 18:29:30,455][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 18:29:30,456][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 18:55:45,810][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpghdzgvip
[2022-06-24 18:55:45,810][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpghdzgvip/_remote_module_non_sriptable.py
[2022-06-24 18:55:46,782][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 18:55:46,786][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 18:55:46,797][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 18:55:46,797][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 18:58:30,223][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp46dns_nw
[2022-06-24 18:58:30,224][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp46dns_nw/_remote_module_non_sriptable.py
[2022-06-24 18:58:31,284][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 18:58:31,285][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 18:58:31,290][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 18:58:31,290][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 18:58:40,609][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-24 18:59:08,766][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-06-24 19:01:27,042][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp4nbdf4as
[2022-06-24 19:01:27,042][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp4nbdf4as/_remote_module_non_sriptable.py
[2022-06-24 19:01:27,998][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 19:01:28,003][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 19:01:28,016][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 19:01:28,016][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 22:18:44,189][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpaesphd4d
[2022-06-24 22:18:44,189][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpaesphd4d/_remote_module_non_sriptable.py
[2022-06-24 22:18:45,121][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 22:18:45,123][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 22:18:45,129][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 22:18:45,129][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 22:18:55,889][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

