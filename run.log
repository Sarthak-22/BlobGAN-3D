[2022-06-22 16:17:45,457][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp6yojwmii
[2022-06-22 16:17:45,457][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp6yojwmii/_remote_module_non_sriptable.py
[2022-06-22 16:17:46,811][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-22 16:17:46,815][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-22 16:17:46,827][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-22 16:17:46,827][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-22 18:58:19,351][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpx557xkub
[2022-06-22 18:58:19,351][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpx557xkub/_remote_module_non_sriptable.py
[2022-06-22 19:01:40,510][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-22 19:01:40,514][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-22 19:01:40,525][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-22 19:01:40,526][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-22 19:10:57,823][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp9gbvatd9
[2022-06-22 19:10:57,824][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp9gbvatd9/_remote_module_non_sriptable.py
[2022-06-22 19:11:40,204][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-22 19:11:40,209][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-22 19:11:40,224][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-22 19:11:40,224][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 13:54:23,308][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpcpv8nupt
[2022-06-23 13:54:23,309][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpcpv8nupt/_remote_module_non_sriptable.py
[2022-06-23 13:54:38,369][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 13:54:38,373][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 13:54:38,384][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 13:54:38,384][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 14:15:39,543][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp5s3tzl_d
[2022-06-23 14:15:39,543][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp5s3tzl_d/_remote_module_non_sriptable.py
[2022-06-23 14:15:40,636][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 14:15:40,639][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 14:15:40,651][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 14:15:40,651][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 17:58:57,593][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpnhaign8a
[2022-06-23 17:58:57,594][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpnhaign8a/_remote_module_non_sriptable.py
[2022-06-23 17:58:58,746][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 17:58:58,749][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 17:58:58,760][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 17:58:58,760][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 18:01:08,409][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpe7uj4hj0
[2022-06-23 18:01:08,410][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpe7uj4hj0/_remote_module_non_sriptable.py
[2022-06-23 18:01:09,429][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 18:01:09,434][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 18:01:09,444][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 18:01:09,444][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 18:19:18,238][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp71hnwpr2
[2022-06-23 18:19:18,239][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp71hnwpr2/_remote_module_non_sriptable.py
[2022-06-23 18:19:19,245][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 18:19:19,249][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 18:19:19,261][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 18:19:19,262][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 18:24:14,928][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpkcncxz12
[2022-06-23 18:24:14,929][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpkcncxz12/_remote_module_non_sriptable.py
[2022-06-23 18:24:16,075][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 18:24:16,079][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 18:24:16,091][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 18:24:16,092][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 18:28:43,508][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp9r5s3lha
[2022-06-23 18:28:43,509][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp9r5s3lha/_remote_module_non_sriptable.py
[2022-06-23 18:28:44,449][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 18:28:44,453][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 18:28:44,464][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 18:28:44,464][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 18:33:00,031][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp13iaeblb
[2022-06-23 18:33:00,031][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp13iaeblb/_remote_module_non_sriptable.py
[2022-06-23 18:33:01,086][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 18:33:01,091][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 18:33:01,104][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 18:33:01,104][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 18:34:42,244][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpnrzvyt13
[2022-06-23 18:34:42,245][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpnrzvyt13/_remote_module_non_sriptable.py
[2022-06-23 18:34:43,260][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-23 18:34:43,264][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-23 18:34:43,275][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-23 18:34:43,275][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-23 18:34:59,907][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-23 18:35:18,592][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-24 17:10:28,708][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpbs79_uar
[2022-06-24 17:10:28,709][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpbs79_uar/_remote_module_non_sriptable.py
[2022-06-24 17:10:29,609][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 17:10:29,611][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 17:10:29,617][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 17:10:29,617][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 17:10:56,178][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-06-24 17:13:29,781][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpqprxxn0c
[2022-06-24 17:13:29,781][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpqprxxn0c/_remote_module_non_sriptable.py
[2022-06-24 17:13:30,986][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 17:13:30,988][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 17:13:30,996][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 17:13:30,996][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 17:14:59,650][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpxcnyjj4u
[2022-06-24 17:14:59,650][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpxcnyjj4u/_remote_module_non_sriptable.py
[2022-06-24 17:15:00,564][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 17:15:00,566][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 17:15:00,574][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 17:15:00,575][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 17:16:57,002][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpzlf4l20d
[2022-06-24 17:16:57,002][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpzlf4l20d/_remote_module_non_sriptable.py
[2022-06-24 17:16:58,009][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 17:16:58,010][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 17:16:58,015][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 17:16:58,015][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 17:17:06,525][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-24 17:17:08,363][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:146: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...
  "`training_step` returned `None`. If this was on purpose, ignore this warning..."

[2022-06-24 17:19:43,645][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-06-24 17:34:50,203][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp_kbcojcr
[2022-06-24 17:34:50,204][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp_kbcojcr/_remote_module_non_sriptable.py
[2022-06-24 17:34:51,153][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 17:34:51,156][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 17:34:51,165][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 17:34:51,165][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 17:34:59,205][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-24 17:35:01,862][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:146: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...
  "`training_step` returned `None`. If this was on purpose, ignore this warning..."

[2022-06-24 17:35:33,491][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-06-24 17:39:02,328][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpugw5d1kh
[2022-06-24 17:39:02,329][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpugw5d1kh/_remote_module_non_sriptable.py
[2022-06-24 17:39:03,245][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 17:39:03,249][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 17:39:03,260][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 17:39:03,261][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 17:39:11,871][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (5) is smaller than the logging interval Trainer(log_every_n_steps=200). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  f"The number of training samples ({self.num_training_batches}) is smaller than the logging interval"

[2022-06-24 18:02:42,618][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpo0mqmby7
[2022-06-24 18:02:42,620][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpo0mqmby7/_remote_module_non_sriptable.py
[2022-06-24 18:02:43,520][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 18:02:43,524][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 18:02:43,535][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 18:02:43,536][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 18:02:52,692][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (5) is smaller than the logging interval Trainer(log_every_n_steps=200). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  f"The number of training samples ({self.num_training_batches}) is smaller than the logging interval"

[2022-06-24 18:09:51,166][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpuk27vrih
[2022-06-24 18:09:51,167][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpuk27vrih/_remote_module_non_sriptable.py
[2022-06-24 18:09:52,151][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 18:09:52,155][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 18:09:52,166][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 18:09:52,166][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 18:11:10,982][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp4k4rvqt9
[2022-06-24 18:11:10,982][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp4k4rvqt9/_remote_module_non_sriptable.py
[2022-06-24 18:11:11,924][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 18:11:11,927][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 18:11:11,938][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 18:11:11,938][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 18:19:25,134][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpfena6eby
[2022-06-24 18:19:25,135][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpfena6eby/_remote_module_non_sriptable.py
[2022-06-24 18:19:26,381][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 18:19:26,382][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 18:19:26,387][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 18:19:26,387][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 18:19:39,213][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-24 18:19:44,290][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:146: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...
  "`training_step` returned `None`. If this was on purpose, ignore this warning..."

[2022-06-24 18:22:15,917][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-06-24 18:25:29,887][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpewivif0o
[2022-06-24 18:25:29,887][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpewivif0o/_remote_module_non_sriptable.py
[2022-06-24 18:25:31,025][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 18:25:31,029][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 18:25:31,040][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 18:25:31,040][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 18:25:54,191][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-24 18:26:11,283][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-24 18:29:29,412][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp1yse3wr4
[2022-06-24 18:29:29,412][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp1yse3wr4/_remote_module_non_sriptable.py
[2022-06-24 18:29:30,441][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 18:29:30,445][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 18:29:30,455][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 18:29:30,456][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 18:55:45,810][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpghdzgvip
[2022-06-24 18:55:45,810][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpghdzgvip/_remote_module_non_sriptable.py
[2022-06-24 18:55:46,782][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 18:55:46,786][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 18:55:46,797][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 18:55:46,797][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 18:58:30,223][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp46dns_nw
[2022-06-24 18:58:30,224][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp46dns_nw/_remote_module_non_sriptable.py
[2022-06-24 18:58:31,284][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 18:58:31,285][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 18:58:31,290][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 18:58:31,290][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 18:58:40,609][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-24 18:59:08,766][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-06-24 19:01:27,042][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp4nbdf4as
[2022-06-24 19:01:27,042][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp4nbdf4as/_remote_module_non_sriptable.py
[2022-06-24 19:01:27,998][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 19:01:28,003][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 19:01:28,016][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 19:01:28,016][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 22:18:44,189][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpaesphd4d
[2022-06-24 22:18:44,189][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpaesphd4d/_remote_module_non_sriptable.py
[2022-06-24 22:18:45,121][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-24 22:18:45,123][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-24 22:18:45,129][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-24 22:18:45,129][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-24 22:18:55,889][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-06-28 16:13:53,661][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpcyh6o89p
[2022-06-28 16:13:53,661][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpcyh6o89p/_remote_module_non_sriptable.py
[2022-06-28 16:14:37,989][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmppbzrr97n
[2022-06-28 16:14:37,990][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmppbzrr97n/_remote_module_non_sriptable.py
[2022-06-28 17:21:01,638][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpn_3jtd4j
[2022-06-28 17:21:01,639][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpn_3jtd4j/_remote_module_non_sriptable.py
[2022-06-28 17:22:46,555][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp5ivatinu
[2022-06-28 17:22:46,556][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp5ivatinu/_remote_module_non_sriptable.py
[2022-06-28 17:22:52,093][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-28 17:22:52,097][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-28 17:22:52,108][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-28 17:22:52,108][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-28 17:25:08,887][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpe36a51ms
[2022-06-28 17:25:08,887][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpe36a51ms/_remote_module_non_sriptable.py
[2022-06-28 17:25:14,740][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-28 17:25:14,743][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-28 17:25:14,754][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-28 17:25:14,754][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-28 17:25:59,978][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpbjmouzqh
[2022-06-28 17:25:59,979][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpbjmouzqh/_remote_module_non_sriptable.py
[2022-06-28 17:26:08,448][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-28 17:26:08,452][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-28 17:26:08,464][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-28 17:26:08,464][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-29 13:36:47,270][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp0j6zzom_
[2022-06-29 13:36:47,271][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp0j6zzom_/_remote_module_non_sriptable.py
[2022-06-29 13:36:55,318][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-29 13:36:55,322][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-29 13:36:55,333][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-29 13:36:55,333][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-29 16:40:25,069][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpixtsyxqd
[2022-06-29 16:40:25,070][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpixtsyxqd/_remote_module_non_sriptable.py
[2022-06-29 16:43:42,775][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpqtwkosq8
[2022-06-29 16:43:42,775][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpqtwkosq8/_remote_module_non_sriptable.py
[2022-06-29 16:43:51,017][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-29 16:43:51,023][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-29 16:43:51,037][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-29 16:43:51,038][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-29 16:47:01,314][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpge6vya6m
[2022-06-29 16:47:01,315][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpge6vya6m/_remote_module_non_sriptable.py
[2022-06-29 16:47:31,127][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-29 16:47:31,131][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-29 16:47:31,141][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-29 16:47:31,142][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-29 17:02:08,950][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp15uhosx7
[2022-06-29 17:02:08,950][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp15uhosx7/_remote_module_non_sriptable.py
[2022-06-29 17:02:10,006][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-29 17:02:10,010][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-29 17:02:10,021][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-29 17:02:10,021][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-29 17:06:44,237][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpysixdnea
[2022-06-29 17:06:44,237][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpysixdnea/_remote_module_non_sriptable.py
[2022-06-29 17:06:45,185][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-29 17:06:45,189][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-29 17:06:45,201][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-29 17:06:45,201][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-29 17:22:09,540][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp_q638y5d
[2022-06-29 17:22:09,540][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp_q638y5d/_remote_module_non_sriptable.py
[2022-06-29 17:22:10,851][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-29 17:22:10,856][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-29 17:22:10,870][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-29 17:22:10,870][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-29 17:26:10,583][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmporf1ssmt
[2022-06-29 17:26:10,584][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmporf1ssmt/_remote_module_non_sriptable.py
[2022-06-29 17:26:11,553][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-29 17:26:11,557][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-29 17:26:11,568][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-29 17:26:11,568][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-29 17:31:19,165][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp5cgjsbwa
[2022-06-29 17:31:19,167][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp5cgjsbwa/_remote_module_non_sriptable.py
[2022-06-29 17:31:20,182][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-29 17:31:20,186][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-29 17:31:20,197][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-29 17:31:20,198][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-30 13:43:53,830][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpo40bljkg
[2022-06-30 13:43:53,831][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpo40bljkg/_remote_module_non_sriptable.py
[2022-06-30 13:43:54,872][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-30 13:43:54,876][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-30 13:43:54,887][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-30 13:43:54,887][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-30 13:47:43,027][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpmwaei2rl
[2022-06-30 13:47:43,027][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpmwaei2rl/_remote_module_non_sriptable.py
[2022-06-30 13:47:43,966][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-30 13:47:43,970][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-30 13:47:43,980][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-30 13:47:43,980][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-30 13:48:39,242][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp6t8csqg7
[2022-06-30 13:48:39,242][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp6t8csqg7/_remote_module_non_sriptable.py
[2022-06-30 13:48:40,165][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-30 13:48:40,169][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-30 13:48:40,180][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-30 13:48:40,181][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-30 13:49:33,005][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp0fvdo2tz
[2022-06-30 13:49:33,006][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp0fvdo2tz/_remote_module_non_sriptable.py
[2022-06-30 13:49:34,150][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-30 13:49:34,154][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-30 13:49:34,167][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-30 13:49:34,167][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-30 15:48:53,404][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp0mb9rflp
[2022-06-30 15:48:53,404][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp0mb9rflp/_remote_module_non_sriptable.py
[2022-06-30 15:48:54,319][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-30 15:48:54,323][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-30 15:48:54,333][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-30 15:48:54,334][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-30 16:15:28,546][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpbq_64i3j
[2022-06-30 16:15:28,547][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpbq_64i3j/_remote_module_non_sriptable.py
[2022-06-30 16:15:29,474][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-30 16:15:29,482][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-30 16:15:29,494][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-30 16:15:29,494][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-30 16:18:06,151][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmphjf2ri2l
[2022-06-30 16:18:06,152][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmphjf2ri2l/_remote_module_non_sriptable.py
[2022-06-30 16:18:07,083][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-30 16:18:07,087][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-30 16:18:07,097][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-30 16:18:07,097][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-30 16:29:33,345][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpyty_mba7
[2022-06-30 16:29:33,346][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpyty_mba7/_remote_module_non_sriptable.py
[2022-06-30 16:29:34,338][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-30 16:29:34,342][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-30 16:29:34,353][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-30 16:29:34,353][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-30 16:33:28,350][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmplfu96v3y
[2022-06-30 16:33:28,350][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmplfu96v3y/_remote_module_non_sriptable.py
[2022-06-30 16:33:29,352][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-30 16:33:29,356][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-30 16:33:29,366][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-30 16:33:29,366][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-30 16:39:32,942][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpqjq01lkn
[2022-06-30 16:39:32,943][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpqjq01lkn/_remote_module_non_sriptable.py
[2022-06-30 16:39:33,859][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-30 16:39:33,863][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-30 16:39:33,874][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-30 16:39:33,874][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-30 17:32:08,477][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpcardj7w7
[2022-06-30 17:32:08,477][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpcardj7w7/_remote_module_non_sriptable.py
[2022-06-30 17:32:09,410][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-30 17:32:09,414][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-30 17:32:09,425][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-30 17:32:09,425][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-30 18:14:53,648][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpreq7wh9w
[2022-06-30 18:14:53,649][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpreq7wh9w/_remote_module_non_sriptable.py
[2022-06-30 18:15:38,143][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp9vd3jxee
[2022-06-30 18:15:38,143][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp9vd3jxee/_remote_module_non_sriptable.py
[2022-06-30 18:15:39,035][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-30 18:15:39,039][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-30 18:15:39,050][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-30 18:15:39,051][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-06-30 18:24:14,852][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp0ru0yku4
[2022-06-30 18:24:14,852][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp0ru0yku4/_remote_module_non_sriptable.py
[2022-06-30 18:26:41,416][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp05lkk34r
[2022-06-30 18:26:41,416][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp05lkk34r/_remote_module_non_sriptable.py
[2022-06-30 18:31:37,738][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpbc6bfz21
[2022-06-30 18:31:37,738][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpbc6bfz21/_remote_module_non_sriptable.py
[2022-06-30 18:39:45,800][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp72gukgz8
[2022-06-30 18:39:45,800][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp72gukgz8/_remote_module_non_sriptable.py
[2022-06-30 18:39:53,246][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-06-30 18:39:53,249][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-06-30 18:39:53,260][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-06-30 18:39:53,261][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-01 16:26:38,728][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpfsfjumge
[2022-07-01 16:26:38,729][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpfsfjumge/_remote_module_non_sriptable.py
[2022-07-01 16:26:40,325][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-01 16:26:40,329][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-01 16:26:40,341][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-01 16:26:40,341][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-01 16:29:44,401][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpqvi7ssqy
[2022-07-01 16:29:44,401][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpqvi7ssqy/_remote_module_non_sriptable.py
[2022-07-01 16:29:46,024][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-01 16:29:46,028][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-01 16:29:46,039][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-01 16:29:46,039][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-01 16:36:03,432][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpri9g5vul
[2022-07-01 16:36:03,433][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpri9g5vul/_remote_module_non_sriptable.py
[2022-07-01 16:36:05,036][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-01 16:36:05,040][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-01 16:36:05,051][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-01 16:36:05,052][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-01 16:43:31,486][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmplh0ikdxv
[2022-07-01 16:43:31,487][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmplh0ikdxv/_remote_module_non_sriptable.py
[2022-07-01 16:43:33,280][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-01 16:43:33,285][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-01 16:43:33,296][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-01 16:43:33,296][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-01 17:06:54,148][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp9dxn2ve6
[2022-07-01 17:06:54,149][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp9dxn2ve6/_remote_module_non_sriptable.py
[2022-07-01 17:06:55,774][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-01 17:06:55,778][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-01 17:06:55,788][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-01 17:06:55,788][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-01 17:55:41,602][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp9k90lt7f
[2022-07-01 17:55:41,603][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp9k90lt7f/_remote_module_non_sriptable.py
[2022-07-01 17:55:43,428][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-01 17:55:43,432][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-01 17:55:43,443][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-01 17:55:43,443][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-02 13:55:06,089][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp_ia48aa5
[2022-07-02 13:55:06,089][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp_ia48aa5/_remote_module_non_sriptable.py
[2022-07-02 13:56:38,578][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp7ut7h85w
[2022-07-02 13:56:38,578][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp7ut7h85w/_remote_module_non_sriptable.py
[2022-07-02 13:57:21,994][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpzb0nx6_q
[2022-07-02 13:57:21,995][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpzb0nx6_q/_remote_module_non_sriptable.py
[2022-07-02 14:03:26,447][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpvan8ly41
[2022-07-02 14:03:26,448][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpvan8ly41/_remote_module_non_sriptable.py
[2022-07-02 14:07:14,997][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpevv0po1m
[2022-07-02 14:07:14,998][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpevv0po1m/_remote_module_non_sriptable.py
[2022-07-02 14:18:19,603][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp7uu2u7_y
[2022-07-02 14:18:19,604][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp7uu2u7_y/_remote_module_non_sriptable.py
[2022-07-04 14:10:51,188][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp90l03f14
[2022-07-04 14:10:51,189][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp90l03f14/_remote_module_non_sriptable.py
[2022-07-04 14:14:51,415][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-04 14:14:51,419][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-04 14:26:15,472][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-04 14:26:15,472][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-04 14:39:35,625][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpk5ftjhfm
[2022-07-04 14:39:35,626][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpk5ftjhfm/_remote_module_non_sriptable.py
[2022-07-04 14:39:42,267][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-04 14:39:42,271][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-04 14:40:02,008][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-04 14:40:02,008][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
[2022-07-04 14:40:59,217][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmprk7ev998
[2022-07-04 14:40:59,218][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmprk7ev998/_remote_module_non_sriptable.py
[2022-07-04 14:41:00,937][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-04 14:41:00,942][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-04 14:41:11,277][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-04 14:41:11,277][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
[2022-07-04 14:41:47,080][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp3pe0pcnf
[2022-07-04 14:41:47,081][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp3pe0pcnf/_remote_module_non_sriptable.py
[2022-07-04 14:41:48,791][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-04 14:41:48,795][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-04 14:41:48,805][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-04 14:41:48,805][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-04 14:56:31,501][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp_3dqvuqj
[2022-07-04 14:56:31,501][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp_3dqvuqj/_remote_module_non_sriptable.py
[2022-07-04 14:56:33,225][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-04 14:56:33,228][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-04 14:56:33,234][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-04 14:56:33,234][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-04 15:05:59,603][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpsr911hgz
[2022-07-04 15:05:59,604][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpsr911hgz/_remote_module_non_sriptable.py
[2022-07-04 15:06:37,607][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-04 15:06:37,612][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-04 15:06:37,623][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-04 15:06:37,623][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-04 16:05:46,442][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpdb1dsndt
[2022-07-04 16:05:46,443][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpdb1dsndt/_remote_module_non_sriptable.py
[2022-07-04 16:05:50,851][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-04 16:05:50,857][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-04 16:05:50,870][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-04 16:05:50,871][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-04 17:18:11,426][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpa_8tzjbm
[2022-07-04 17:18:11,427][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpa_8tzjbm/_remote_module_non_sriptable.py
[2022-07-04 17:18:14,902][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-04 17:18:14,908][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-04 17:18:14,921][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-04 17:18:14,921][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-04 17:23:46,474][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpd78j25y9
[2022-07-04 17:23:46,475][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpd78j25y9/_remote_module_non_sriptable.py
[2022-07-04 17:25:37,848][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp_ocyteap
[2022-07-04 17:25:37,848][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp_ocyteap/_remote_module_non_sriptable.py
[2022-07-04 17:25:41,225][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-04 17:25:41,230][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-04 17:25:41,244][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-04 17:25:41,244][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-04 17:30:06,424][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmptd4ndx1_
[2022-07-04 17:30:06,425][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmptd4ndx1_/_remote_module_non_sriptable.py
[2022-07-04 17:30:09,224][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-04 17:30:09,229][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-04 17:30:09,241][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-04 17:30:09,242][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-04 17:31:28,100][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpg6tk7dii
[2022-07-04 17:31:28,100][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpg6tk7dii/_remote_module_non_sriptable.py
[2022-07-04 17:31:31,041][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-04 17:31:31,045][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-04 17:31:31,056][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-04 17:31:31,056][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-04 17:36:57,165][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpq9jkndby
[2022-07-04 17:36:57,165][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpq9jkndby/_remote_module_non_sriptable.py
[2022-07-04 17:37:00,859][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-04 17:37:00,864][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-04 17:37:00,875][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-04 17:37:00,876][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-04 17:38:12,796][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpsypo7iok
[2022-07-04 17:38:12,797][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpsypo7iok/_remote_module_non_sriptable.py
[2022-07-04 17:38:16,287][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-04 17:38:16,299][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-04 17:38:16,315][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-04 17:38:16,315][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-04 17:54:03,729][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpz2zo7glz
[2022-07-04 17:54:03,729][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpz2zo7glz/_remote_module_non_sriptable.py
[2022-07-04 17:54:06,630][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-04 17:54:06,636][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-04 17:54:45,713][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-04 17:54:45,713][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-04 18:02:35,811][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpsj3l5368
[2022-07-04 18:02:35,813][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpsj3l5368/_remote_module_non_sriptable.py
[2022-07-04 18:02:39,148][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-04 18:02:39,155][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-04 18:02:39,167][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-04 18:02:39,167][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-04 18:04:23,224][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp7uhj6r7t
[2022-07-04 18:04:23,225][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp7uhj6r7t/_remote_module_non_sriptable.py
[2022-07-04 18:04:27,684][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-04 18:04:27,688][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-04 18:04:27,700][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-04 18:04:27,700][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 14:31:17,428][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp8bu9pitd
[2022-07-05 14:31:17,428][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp8bu9pitd/_remote_module_non_sriptable.py
[2022-07-05 14:31:19,402][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 14:31:19,406][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 14:31:19,416][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 14:31:19,416][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 14:39:47,975][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp2ndr3y5k
[2022-07-05 14:39:47,976][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp2ndr3y5k/_remote_module_non_sriptable.py
[2022-07-05 14:39:50,429][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 14:39:50,434][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 14:39:50,451][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 14:39:50,451][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 14:40:49,537][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmphfd5p4n4
[2022-07-05 14:40:49,537][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmphfd5p4n4/_remote_module_non_sriptable.py
[2022-07-05 14:40:51,566][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 14:40:51,571][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 14:40:51,582][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 14:40:51,582][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 14:42:03,582][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpey4y2m13
[2022-07-05 14:42:03,583][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpey4y2m13/_remote_module_non_sriptable.py
[2022-07-05 14:42:05,506][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 14:42:05,510][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 14:42:05,521][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 14:42:05,521][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 15:02:07,359][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpuo_9zeph
[2022-07-05 15:02:07,360][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpuo_9zeph/_remote_module_non_sriptable.py
[2022-07-05 15:02:09,151][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 15:02:09,155][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 15:02:09,166][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 15:02:09,167][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 15:03:19,481][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpg177eaa3
[2022-07-05 15:03:19,482][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpg177eaa3/_remote_module_non_sriptable.py
[2022-07-05 15:03:21,331][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 15:03:21,335][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 15:03:21,345][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 15:03:21,345][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 15:05:19,251][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 15:06:07,432][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpsnsiu38x
[2022-07-05 15:06:07,433][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpsnsiu38x/_remote_module_non_sriptable.py
[2022-07-05 15:06:09,575][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 15:06:09,579][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 15:06:09,590][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 15:06:09,591][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 15:06:43,665][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 15:11:38,351][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Error detected in Conv2dBackward. Traceback of forward call that caused the error:
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/__main__.py", line 45, in <module>
    cli.main()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 444, in main
    run()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 285, in run_file
    runpy.run_path(target_as_str, run_name=compat.force_str("__main__"))
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 90, in <module>
    run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/main.py", line 52, in decorated_main
    config_name=config_name,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 378, in _run_hydra
    lambda: hydra.run(
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 211, in run_and_report
    return func()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 381, in <lambda>
    overrides=args.overrides,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/hydra.py", line 106, in run
    configure_logging=with_log_configuration,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/core/utils.py", line 160, in run_job
    ret.return_value = task_function(task_cfg)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 82, in run
    trainer.fit(model, datamodule=datamodule)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1289, in run_stage
    return self._run_train()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1319, in _run_train
    self.fit_loop.run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 234, in advance
    self.epoch_loop.run(data_fetcher)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 193, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 219, in advance
    self.optimizer_idx,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 266, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 386, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 226, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 164, in step
    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 339, in optimizer_step
    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 163, in optimizer_step
    optimizer.step(closure=closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/adamw.py", line 100, in step
    loss = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 148, in _wrap_closure
    closure_result = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 160, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 142, in closure
    step_output = self._step_fn()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 435, in _training_step
    training_step_output = self.trainer.accelerator.training_step(step_kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 219, in training_step
    return self.training_type_plugin.training_step(*step_kwargs.values())
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 439, in training_step
    return self.model(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/overrides/base.py", line 81, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/base.py", line 20, in training_step
    return self.shared_step(batch, batch_idx, optimizer_idx, 'train')
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 754, in shared_step
    logits_fake['top'] = self.discriminator['top'](gen_imgs['top'])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 777, in forward
    out = self.final_conv(out)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 162, in forward
    padding=self.padding,
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/op/conv2d_gradfix_111andon.py", line 41, in conv2d
    output_padding=0, dilation=dilation, groups=groups).apply(input, weight, bias)
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass

[2022-07-05 15:13:26,517][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp94p9c1vg
[2022-07-05 15:13:26,517][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp94p9c1vg/_remote_module_non_sriptable.py
[2022-07-05 15:13:28,237][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 15:13:28,241][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 15:13:28,252][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 15:13:28,252][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 15:16:49,077][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpb6uzjv0v
[2022-07-05 15:16:49,078][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpb6uzjv0v/_remote_module_non_sriptable.py
[2022-07-05 15:16:51,117][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 15:16:51,121][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 15:16:51,132][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 15:16:51,132][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 15:19:04,776][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpoazd0u0z
[2022-07-05 15:19:04,777][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpoazd0u0z/_remote_module_non_sriptable.py
[2022-07-05 15:19:06,686][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 15:19:06,690][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 15:19:06,701][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 15:19:06,701][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 15:19:21,765][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 15:19:43,937][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Error detected in UpFirDn2dBackward. Traceback of forward call that caused the error:
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/__main__.py", line 45, in <module>
    cli.main()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 444, in main
    run()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 285, in run_file
    runpy.run_path(target_as_str, run_name=compat.force_str("__main__"))
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 90, in <module>
    run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/main.py", line 52, in decorated_main
    config_name=config_name,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 378, in _run_hydra
    lambda: hydra.run(
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 211, in run_and_report
    return func()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 381, in <lambda>
    overrides=args.overrides,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/hydra.py", line 106, in run
    configure_logging=with_log_configuration,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/core/utils.py", line 160, in run_job
    ret.return_value = task_function(task_cfg)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 82, in run
    trainer.fit(model, datamodule=datamodule)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1289, in run_stage
    return self._run_train()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1319, in _run_train
    self.fit_loop.run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 234, in advance
    self.epoch_loop.run(data_fetcher)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 193, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 219, in advance
    self.optimizer_idx,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 266, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 386, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 226, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 164, in step
    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 339, in optimizer_step
    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 163, in optimizer_step
    optimizer.step(closure=closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/adamw.py", line 100, in step
    loss = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 148, in _wrap_closure
    closure_result = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 160, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 142, in closure
    step_output = self._step_fn()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 435, in _training_step
    training_step_output = self.trainer.accelerator.training_step(step_kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 219, in training_step
    return self.training_type_plugin.training_step(*step_kwargs.values())
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 439, in training_step
    return self.model(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/overrides/base.py", line 81, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/base.py", line 20, in training_step
    return self.shared_step(batch, batch_idx, optimizer_idx, 'train')
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 754, in shared_step
    logits_fake['top'] = self.discriminator['top'](gen_imgs['top'])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 764, in forward
    out = self.convs(input)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 710, in forward
    skip = self.skip(input)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 91, in forward
    out = upfirdn2d(input, self.kernel, pad=self.pad)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/op/upfirdn2d.py", line 163, in upfirdn2d
    out = UpFirDn2d.apply(input, kernel, up, down, pad)
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass

[2022-07-05 15:22:41,443][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp2jagzbhb
[2022-07-05 15:22:41,444][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp2jagzbhb/_remote_module_non_sriptable.py
[2022-07-05 15:22:43,231][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 15:22:43,235][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 15:22:43,246][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 15:22:43,246][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 15:22:55,918][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 15:23:03,610][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 15:30:53,808][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp5unph5h1
[2022-07-05 15:30:53,809][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp5unph5h1/_remote_module_non_sriptable.py
[2022-07-05 15:30:55,641][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 15:30:55,645][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 15:30:55,656][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 15:30:55,656][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 15:31:09,422][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 15:31:21,515][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 15:33:12,310][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpiqkn4r32
[2022-07-05 15:33:12,310][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpiqkn4r32/_remote_module_non_sriptable.py
[2022-07-05 15:33:14,206][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 15:33:14,210][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 15:33:14,221][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 15:33:14,221][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 15:33:29,153][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 15:33:39,213][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 15:33:53,022][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 15:33:56,500][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Error detected in FusedLeakyReLUFunctionBackward. Traceback of forward call that caused the error:
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/__main__.py", line 45, in <module>
    cli.main()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 444, in main
    run()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 285, in run_file
    runpy.run_path(target_as_str, run_name=compat.force_str("__main__"))
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 90, in <module>
    run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/main.py", line 52, in decorated_main
    config_name=config_name,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 378, in _run_hydra
    lambda: hydra.run(
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 211, in run_and_report
    return func()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 381, in <lambda>
    overrides=args.overrides,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/hydra.py", line 106, in run
    configure_logging=with_log_configuration,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/core/utils.py", line 160, in run_job
    ret.return_value = task_function(task_cfg)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 82, in run
    trainer.fit(model, datamodule=datamodule)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1289, in run_stage
    return self._run_train()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1319, in _run_train
    self.fit_loop.run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 234, in advance
    self.epoch_loop.run(data_fetcher)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 193, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 219, in advance
    self.optimizer_idx,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 266, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 386, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 226, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 164, in step
    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 339, in optimizer_step
    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 163, in optimizer_step
    optimizer.step(closure=closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/adamw.py", line 100, in step
    loss = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 148, in _wrap_closure
    closure_result = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 160, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 142, in closure
    step_output = self._step_fn()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 435, in _training_step
    training_step_output = self.trainer.accelerator.training_step(step_kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 219, in training_step
    return self.training_type_plugin.training_step(*step_kwargs.values())
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 439, in training_step
    return self.model(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/overrides/base.py", line 81, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/base.py", line 20, in training_step
    return self.shared_step(batch, batch_idx, optimizer_idx, 'train')
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 793, in shared_step
    logits_real['top'] = self.discriminator['top'](batch_real)  # Change batch real
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 780, in forward
    out = self.final_linear(out)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 199, in forward
    out = fused_leaky_relu(out, self.bias * self.lr_mul)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/op/fused_act.py", line 119, in fused_leaky_relu
    return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass

[2022-07-05 15:47:32,478][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpr5jwss37
[2022-07-05 15:47:32,479][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpr5jwss37/_remote_module_non_sriptable.py
[2022-07-05 15:47:33,954][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 15:47:33,958][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 15:47:33,963][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 15:47:33,963][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 15:47:45,372][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 15:47:49,648][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Error detected in FusedLeakyReLUFunctionBackward. Traceback of forward call that caused the error:
  File "src/run.py", line 90, in <module>
    run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/main.py", line 52, in decorated_main
    config_name=config_name,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 378, in _run_hydra
    lambda: hydra.run(
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 211, in run_and_report
    return func()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 381, in <lambda>
    overrides=args.overrides,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/hydra.py", line 106, in run
    configure_logging=with_log_configuration,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/core/utils.py", line 160, in run_job
    ret.return_value = task_function(task_cfg)
  File "src/run.py", line 82, in run
    trainer.fit(model, datamodule=datamodule)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1289, in run_stage
    return self._run_train()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1319, in _run_train
    self.fit_loop.run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 234, in advance
    self.epoch_loop.run(data_fetcher)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 193, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 219, in advance
    self.optimizer_idx,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 266, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 386, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 226, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 164, in step
    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 339, in optimizer_step
    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 163, in optimizer_step
    optimizer.step(closure=closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/adamw.py", line 100, in step
    loss = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 148, in _wrap_closure
    closure_result = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 160, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 142, in closure
    step_output = self._step_fn()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 435, in _training_step
    training_step_output = self.trainer.accelerator.training_step(step_kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 219, in training_step
    return self.training_type_plugin.training_step(*step_kwargs.values())
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 439, in training_step
    return self.model(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/overrides/base.py", line 81, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/base.py", line 20, in training_step
    return self.shared_step(batch, batch_idx, optimizer_idx, 'train')
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 793, in shared_step
    logits_real['top'] = self.discriminator['top'](batch_real)  # Change batch real
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 780, in forward
    out = self.final_linear(out)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 199, in forward
    out = fused_leaky_relu(out, self.bias * self.lr_mul)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/op/fused_act.py", line 119, in fused_leaky_relu
    return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass

[2022-07-05 15:53:52,225][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmptb_pw9zg
[2022-07-05 15:53:52,226][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmptb_pw9zg/_remote_module_non_sriptable.py
[2022-07-05 15:53:53,948][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 15:53:53,952][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 15:53:53,963][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 15:53:53,963][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 15:54:53,131][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpducgc5cy
[2022-07-05 15:54:53,132][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpducgc5cy/_remote_module_non_sriptable.py
[2022-07-05 15:54:54,977][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 15:54:54,981][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 15:54:54,991][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 15:54:54,991][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 16:01:22,517][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpmirw3a_l
[2022-07-05 16:01:22,518][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpmirw3a_l/_remote_module_non_sriptable.py
[2022-07-05 16:01:24,315][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 16:01:24,319][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 16:01:24,329][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 16:01:24,329][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 16:02:23,923][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpqywbn2dv
[2022-07-05 16:02:23,924][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpqywbn2dv/_remote_module_non_sriptable.py
[2022-07-05 16:02:26,050][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 16:02:26,054][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 16:02:26,066][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 16:02:26,067][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 16:06:07,049][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpsk8nq0rh
[2022-07-05 16:06:07,050][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpsk8nq0rh/_remote_module_non_sriptable.py
[2022-07-05 16:06:08,842][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 16:06:08,849][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 16:06:08,872][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 16:06:08,872][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 16:14:56,369][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpc16vjljd
[2022-07-05 16:14:56,369][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpc16vjljd/_remote_module_non_sriptable.py
[2022-07-05 16:14:58,475][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 16:14:58,479][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 16:14:58,494][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 16:14:58,494][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 16:15:14,567][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 16:15:34,284][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 16:15:40,987][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 16:15:42,984][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Error detected in FusedLeakyReLUFunctionBackward. Traceback of forward call that caused the error:
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/__main__.py", line 45, in <module>
    cli.main()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 444, in main
    run()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 285, in run_file
    runpy.run_path(target_as_str, run_name=compat.force_str("__main__"))
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 90, in <module>
    run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/main.py", line 52, in decorated_main
    config_name=config_name,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 378, in _run_hydra
    lambda: hydra.run(
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 211, in run_and_report
    return func()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 381, in <lambda>
    overrides=args.overrides,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/hydra.py", line 106, in run
    configure_logging=with_log_configuration,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/core/utils.py", line 160, in run_job
    ret.return_value = task_function(task_cfg)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 82, in run
    trainer.fit(model, datamodule=datamodule)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1289, in run_stage
    return self._run_train()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1319, in _run_train
    self.fit_loop.run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 234, in advance
    self.epoch_loop.run(data_fetcher)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 193, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 219, in advance
    self.optimizer_idx,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 266, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 386, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 226, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 164, in step
    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 339, in optimizer_step
    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 163, in optimizer_step
    optimizer.step(closure=closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/adamw.py", line 100, in step
    loss = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 148, in _wrap_closure
    closure_result = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 160, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 142, in closure
    step_output = self._step_fn()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 435, in _training_step
    training_step_output = self.trainer.accelerator.training_step(step_kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 219, in training_step
    return self.training_type_plugin.training_step(*step_kwargs.values())
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 439, in training_step
    return self.model(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/overrides/base.py", line 81, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/base.py", line 20, in training_step
    return self.shared_step(batch, batch_idx, optimizer_idx, 'train')
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 793, in shared_step
    logits_real['top'] = self.discriminator['top'](batch_real)  # Change batch real
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 780, in forward
    out = self.final_linear(out)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 199, in forward
    out = fused_leaky_relu(out, self.bias * self.lr_mul)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/op/fused_act.py", line 119, in fused_leaky_relu
    return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass

[2022-07-05 16:22:46,373][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpyf7fpte3
[2022-07-05 16:22:46,374][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpyf7fpte3/_remote_module_non_sriptable.py
[2022-07-05 16:22:48,041][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 16:22:48,045][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 16:22:48,057][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 16:22:48,057][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 16:23:02,997][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 16:23:11,465][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 16:23:18,151][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 16:23:20,047][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Error detected in FusedLeakyReLUFunctionBackward. Traceback of forward call that caused the error:
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/__main__.py", line 45, in <module>
    cli.main()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 444, in main
    run()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 285, in run_file
    runpy.run_path(target_as_str, run_name=compat.force_str("__main__"))
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 90, in <module>
    run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/main.py", line 52, in decorated_main
    config_name=config_name,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 378, in _run_hydra
    lambda: hydra.run(
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 211, in run_and_report
    return func()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 381, in <lambda>
    overrides=args.overrides,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/hydra.py", line 106, in run
    configure_logging=with_log_configuration,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/core/utils.py", line 160, in run_job
    ret.return_value = task_function(task_cfg)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 82, in run
    trainer.fit(model, datamodule=datamodule)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1289, in run_stage
    return self._run_train()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1319, in _run_train
    self.fit_loop.run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 234, in advance
    self.epoch_loop.run(data_fetcher)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 193, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 219, in advance
    self.optimizer_idx,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 266, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 386, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 226, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 164, in step
    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 339, in optimizer_step
    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 163, in optimizer_step
    optimizer.step(closure=closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/adamw.py", line 100, in step
    loss = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 148, in _wrap_closure
    closure_result = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 160, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 142, in closure
    step_output = self._step_fn()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 435, in _training_step
    training_step_output = self.trainer.accelerator.training_step(step_kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 219, in training_step
    return self.training_type_plugin.training_step(*step_kwargs.values())
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 439, in training_step
    return self.model(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/overrides/base.py", line 81, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/base.py", line 20, in training_step
    return self.shared_step(batch, batch_idx, optimizer_idx, 'train')
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 793, in shared_step
    logits_real['top'] = self.discriminator['top'](batch_real)  # Change batch real
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 780, in forward
    out = self.final_linear(out)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 199, in forward
    out = fused_leaky_relu(out, self.bias * self.lr_mul)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/op/fused_act.py", line 119, in fused_leaky_relu
    return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass

[2022-07-05 16:25:18,183][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpc38higxd
[2022-07-05 16:25:18,184][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpc38higxd/_remote_module_non_sriptable.py
[2022-07-05 16:25:19,885][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 16:25:19,889][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 16:25:19,899][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 16:25:19,899][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 16:25:31,171][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 16:25:35,968][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Error detected in FusedLeakyReLUFunctionBackward. Traceback of forward call that caused the error:
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/__main__.py", line 45, in <module>
    cli.main()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 444, in main
    run()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 285, in run_file
    runpy.run_path(target_as_str, run_name=compat.force_str("__main__"))
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 90, in <module>
    run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/main.py", line 52, in decorated_main
    config_name=config_name,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 378, in _run_hydra
    lambda: hydra.run(
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 211, in run_and_report
    return func()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 381, in <lambda>
    overrides=args.overrides,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/hydra.py", line 106, in run
    configure_logging=with_log_configuration,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/core/utils.py", line 160, in run_job
    ret.return_value = task_function(task_cfg)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 82, in run
    trainer.fit(model, datamodule=datamodule)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1289, in run_stage
    return self._run_train()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1319, in _run_train
    self.fit_loop.run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 234, in advance
    self.epoch_loop.run(data_fetcher)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 193, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 219, in advance
    self.optimizer_idx,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 266, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 386, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 226, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 164, in step
    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 339, in optimizer_step
    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 163, in optimizer_step
    optimizer.step(closure=closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/adamw.py", line 100, in step
    loss = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 148, in _wrap_closure
    closure_result = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 160, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 142, in closure
    step_output = self._step_fn()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 435, in _training_step
    training_step_output = self.trainer.accelerator.training_step(step_kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 219, in training_step
    return self.training_type_plugin.training_step(*step_kwargs.values())
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 439, in training_step
    return self.model(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/overrides/base.py", line 81, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/base.py", line 20, in training_step
    return self.shared_step(batch, batch_idx, optimizer_idx, 'train')
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 793, in shared_step
    logits_real['top'] = self.discriminator['top'](batch_real)  # Change batch real
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 780, in forward
    out = self.final_linear(out)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 199, in forward
    out = fused_leaky_relu(out, self.bias * self.lr_mul)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/op/fused_act.py", line 119, in fused_leaky_relu
    return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass

[2022-07-05 16:26:44,446][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp0kucijpm
[2022-07-05 16:26:44,446][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp0kucijpm/_remote_module_non_sriptable.py
[2022-07-05 16:26:46,129][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 16:26:46,133][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 16:26:46,144][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 16:26:46,144][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 16:26:55,512][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 16:27:01,962][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Error detected in FusedLeakyReLUFunctionBackward. Traceback of forward call that caused the error:
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/__main__.py", line 45, in <module>
    cli.main()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 444, in main
    run()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 285, in run_file
    runpy.run_path(target_as_str, run_name=compat.force_str("__main__"))
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 90, in <module>
    run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/main.py", line 52, in decorated_main
    config_name=config_name,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 378, in _run_hydra
    lambda: hydra.run(
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 211, in run_and_report
    return func()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 381, in <lambda>
    overrides=args.overrides,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/hydra.py", line 106, in run
    configure_logging=with_log_configuration,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/core/utils.py", line 160, in run_job
    ret.return_value = task_function(task_cfg)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 82, in run
    trainer.fit(model, datamodule=datamodule)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1289, in run_stage
    return self._run_train()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1319, in _run_train
    self.fit_loop.run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 234, in advance
    self.epoch_loop.run(data_fetcher)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 193, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 219, in advance
    self.optimizer_idx,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 266, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 386, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 226, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 164, in step
    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 339, in optimizer_step
    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 163, in optimizer_step
    optimizer.step(closure=closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/adamw.py", line 100, in step
    loss = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 148, in _wrap_closure
    closure_result = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 160, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 142, in closure
    step_output = self._step_fn()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 435, in _training_step
    training_step_output = self.trainer.accelerator.training_step(step_kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 219, in training_step
    return self.training_type_plugin.training_step(*step_kwargs.values())
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 439, in training_step
    return self.model(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/overrides/base.py", line 81, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/base.py", line 20, in training_step
    return self.shared_step(batch, batch_idx, optimizer_idx, 'train')
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 793, in shared_step
    logits_real['top'] = self.discriminator['top'](batch_real)  # Change batch real
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 780, in forward
    out = self.final_linear(out)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 199, in forward
    out = fused_leaky_relu(out, self.bias * self.lr_mul)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/op/fused_act.py", line 119, in fused_leaky_relu
    return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass

[2022-07-05 16:32:05,951][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpimapmli4
[2022-07-05 16:32:05,952][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpimapmli4/_remote_module_non_sriptable.py
[2022-07-05 16:32:08,018][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 16:32:08,022][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 16:32:08,032][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 16:32:08,032][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 16:34:08,784][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmppqhtinbu
[2022-07-05 16:34:08,784][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmppqhtinbu/_remote_module_non_sriptable.py
[2022-07-05 16:34:10,489][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 16:34:10,493][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 16:34:10,503][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 16:34:10,504][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 16:34:26,552][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 16:34:27,175][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Error detected in FusedLeakyReLUFunctionBackward. Traceback of forward call that caused the error:
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/__main__.py", line 45, in <module>
    cli.main()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 444, in main
    run()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 285, in run_file
    runpy.run_path(target_as_str, run_name=compat.force_str("__main__"))
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 90, in <module>
    run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/main.py", line 52, in decorated_main
    config_name=config_name,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 378, in _run_hydra
    lambda: hydra.run(
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 211, in run_and_report
    return func()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 381, in <lambda>
    overrides=args.overrides,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/hydra.py", line 106, in run
    configure_logging=with_log_configuration,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/core/utils.py", line 160, in run_job
    ret.return_value = task_function(task_cfg)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 82, in run
    trainer.fit(model, datamodule=datamodule)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1289, in run_stage
    return self._run_train()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1319, in _run_train
    self.fit_loop.run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 234, in advance
    self.epoch_loop.run(data_fetcher)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 193, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 219, in advance
    self.optimizer_idx,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 266, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 386, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 226, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 164, in step
    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 339, in optimizer_step
    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 163, in optimizer_step
    optimizer.step(closure=closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/adamw.py", line 100, in step
    loss = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 148, in _wrap_closure
    closure_result = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 160, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 142, in closure
    step_output = self._step_fn()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 435, in _training_step
    training_step_output = self.trainer.accelerator.training_step(step_kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 219, in training_step
    return self.training_type_plugin.training_step(*step_kwargs.values())
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 439, in training_step
    return self.model(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/overrides/base.py", line 81, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/base.py", line 20, in training_step
    return self.shared_step(batch, batch_idx, optimizer_idx, 'train')
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 741, in shared_step
    layout, gen_imgs['top'], latents['top'], gen_imgs['front'], latents['front']  = self.gen(z, ret_layout=True, ret_latents=True, viz=log_images)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 261, in gen
    out['top']  = G['top'](**gen_input['top'])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/layoutstylegan.py", line 213, in forward
    out = conv2(out, latent[i + 1], noise=noise2)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/layoutstylegan.py", line 285, in forward
    out = self.activate(out)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/op/fused_act.py", line 101, in forward
    return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/op/fused_act.py", line 119, in fused_leaky_relu
    return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass

[2022-07-05 16:48:38,556][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpzisjj6wi
[2022-07-05 16:48:38,557][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpzisjj6wi/_remote_module_non_sriptable.py
[2022-07-05 16:48:40,345][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 16:48:40,349][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 16:48:40,360][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 16:48:40,360][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 16:52:48,018][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpz19bh3b4
[2022-07-05 16:52:48,018][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpz19bh3b4/_remote_module_non_sriptable.py
[2022-07-05 16:52:49,912][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 16:52:49,916][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 16:52:49,927][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 16:52:49,927][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 16:55:28,737][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpf3kr5j80
[2022-07-05 16:55:28,738][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpf3kr5j80/_remote_module_non_sriptable.py
[2022-07-05 16:55:30,578][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 16:55:30,583][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 16:55:30,594][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 16:55:30,594][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 16:55:47,155][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 16:55:55,435][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Error detected in Conv2dBackward. Traceback of forward call that caused the error:
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/__main__.py", line 45, in <module>
    cli.main()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 444, in main
    run()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 285, in run_file
    runpy.run_path(target_as_str, run_name=compat.force_str("__main__"))
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 90, in <module>
    run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/main.py", line 52, in decorated_main
    config_name=config_name,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 378, in _run_hydra
    lambda: hydra.run(
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 211, in run_and_report
    return func()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 381, in <lambda>
    overrides=args.overrides,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/hydra.py", line 106, in run
    configure_logging=with_log_configuration,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/core/utils.py", line 160, in run_job
    ret.return_value = task_function(task_cfg)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 82, in run
    trainer.fit(model, datamodule=datamodule)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1289, in run_stage
    return self._run_train()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1319, in _run_train
    self.fit_loop.run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 234, in advance
    self.epoch_loop.run(data_fetcher)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 193, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 219, in advance
    self.optimizer_idx,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 266, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 386, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 226, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 164, in step
    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 339, in optimizer_step
    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 163, in optimizer_step
    optimizer.step(closure=closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/adamw.py", line 100, in step
    loss = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 148, in _wrap_closure
    closure_result = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 160, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 142, in closure
    step_output = self._step_fn()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 435, in _training_step
    training_step_output = self.trainer.accelerator.training_step(step_kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 219, in training_step
    return self.training_type_plugin.training_step(*step_kwargs.values())
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 439, in training_step
    return self.model(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/overrides/base.py", line 81, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/base.py", line 20, in training_step
    return self.shared_step(batch, batch_idx, optimizer_idx, 'train')
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 793, in shared_step
    logits_real['top'] = self.discriminator['top'](batch_real)  # Change batch real
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 777, in forward
    out = self.final_conv(out)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 162, in forward
    padding=self.padding,
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/op/conv2d_gradfix_111andon.py", line 41, in conv2d
    output_padding=0, dilation=dilation, groups=groups).apply(input, weight, bias)
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass

[2022-07-05 17:04:45,151][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpd7_nra32
[2022-07-05 17:04:45,151][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpd7_nra32/_remote_module_non_sriptable.py
[2022-07-05 17:04:46,969][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 17:04:46,973][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 17:04:46,983][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 17:04:46,983][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 17:07:59,880][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp4e936cmc
[2022-07-05 17:07:59,881][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp4e936cmc/_remote_module_non_sriptable.py
[2022-07-05 17:08:01,583][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 17:08:01,587][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 17:08:01,597][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 17:08:01,597][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 17:08:18,121][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 17:08:18,885][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Error detected in UpFirDn2dBackward. Traceback of forward call that caused the error:
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/__main__.py", line 45, in <module>
    cli.main()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 444, in main
    run()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 285, in run_file
    runpy.run_path(target_as_str, run_name=compat.force_str("__main__"))
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 90, in <module>
    run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/main.py", line 52, in decorated_main
    config_name=config_name,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 378, in _run_hydra
    lambda: hydra.run(
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 211, in run_and_report
    return func()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 381, in <lambda>
    overrides=args.overrides,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/hydra.py", line 106, in run
    configure_logging=with_log_configuration,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/core/utils.py", line 160, in run_job
    ret.return_value = task_function(task_cfg)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 82, in run
    trainer.fit(model, datamodule=datamodule)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1289, in run_stage
    return self._run_train()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1319, in _run_train
    self.fit_loop.run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 234, in advance
    self.epoch_loop.run(data_fetcher)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 193, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 219, in advance
    self.optimizer_idx,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 266, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 386, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 226, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 164, in step
    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 339, in optimizer_step
    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 163, in optimizer_step
    optimizer.step(closure=closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/adamw.py", line 100, in step
    loss = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 148, in _wrap_closure
    closure_result = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 160, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 142, in closure
    step_output = self._step_fn()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 435, in _training_step
    training_step_output = self.trainer.accelerator.training_step(step_kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 219, in training_step
    return self.training_type_plugin.training_step(*step_kwargs.values())
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 439, in training_step
    return self.model(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/overrides/base.py", line 81, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/base.py", line 20, in training_step
    return self.shared_step(batch, batch_idx, optimizer_idx, 'train')
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 754, in shared_step
    logits_fake['top'] = self.discriminator['top'](gen_imgs['top'])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 764, in forward
    out = self.convs(input)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 710, in forward
    skip = self.skip(input)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 91, in forward
    out = upfirdn2d(input, self.kernel, pad=self.pad)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/op/upfirdn2d.py", line 163, in upfirdn2d
    out = UpFirDn2d.apply(input, kernel, up, down, pad)
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass

[2022-07-05 17:09:16,756][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp7tk6koj1
[2022-07-05 17:09:16,756][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp7tk6koj1/_remote_module_non_sriptable.py
[2022-07-05 17:09:18,945][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 17:09:18,950][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 17:09:18,962][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 17:09:18,963][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 17:10:06,149][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp7eo7vddg
[2022-07-05 17:10:06,149][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp7eo7vddg/_remote_module_non_sriptable.py
[2022-07-05 17:10:08,082][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 17:10:08,086][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 17:10:08,096][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 17:10:08,096][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 17:10:28,883][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 17:10:30,055][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Error detected in UpFirDn2dBackward. Traceback of forward call that caused the error:
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/__main__.py", line 45, in <module>
    cli.main()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 444, in main
    run()
  File "/home/rishubh/.vscode-server/extensions/ms-python.python-2022.8.1/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py", line 285, in run_file
    runpy.run_path(target_as_str, run_name=compat.force_str("__main__"))
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 90, in <module>
    run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/main.py", line 52, in decorated_main
    config_name=config_name,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 378, in _run_hydra
    lambda: hydra.run(
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 211, in run_and_report
    return func()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/utils.py", line 381, in <lambda>
    overrides=args.overrides,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/_internal/hydra.py", line 106, in run
    configure_logging=with_log_configuration,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/hydra/core/utils.py", line 160, in run_job
    ret.return_value = task_function(task_cfg)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/run.py", line 82, in run
    trainer.fit(model, datamodule=datamodule)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1289, in run_stage
    return self._run_train()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1319, in _run_train
    self.fit_loop.run()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py", line 234, in advance
    self.epoch_loop.run(data_fetcher)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 193, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 219, in advance
    self.optimizer_idx,
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 266, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 386, in _optimizer_step
    using_lbfgs=is_lbfgs,
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 226, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py", line 164, in step
    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 339, in optimizer_step
    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 163, in optimizer_step
    optimizer.step(closure=closure, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/optim/adamw.py", line 100, in step
    loss = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 148, in _wrap_closure
    closure_result = closure()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 160, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 142, in closure
    step_output = self._step_fn()
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 435, in _training_step
    training_step_output = self.trainer.accelerator.training_step(step_kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 219, in training_step
    return self.training_type_plugin.training_step(*step_kwargs.values())
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 439, in training_step
    return self.model(*args, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/overrides/base.py", line 81, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/base.py", line 20, in training_step
    return self.shared_step(batch, batch_idx, optimizer_idx, 'train')
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 741, in shared_step
    layout, gen_imgs['top'], latents['top'], gen_imgs['front'], latents['front']  = self.gen(z, ret_layout=True, ret_latents=True, viz=log_images)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/blobgan3d.py", line 261, in gen
    out['top']  = G['top'](**gen_input['top'])
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/layoutstylegan.py", line 215, in forward
    skip = to_rgb(out, latent[i + 2], skip)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/layoutstylegan.py", line 246, in forward
    skip = self.upsample(skip)
  File "/raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/stylegan.py", line 51, in forward
    out = upfirdn2d(input, self.kernel, up=self.factor, down=1, pad=self.pad)
  File "/raid/rishubh/interns/sarthakv/BlobGAN-3D/src/models/networks/op/upfirdn2d.py", line 163, in upfirdn2d
    out = UpFirDn2d.apply(input, kernel, up, down, pad)
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass

[2022-07-05 17:12:37,327][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpu5gfk252
[2022-07-05 17:12:37,328][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpu5gfk252/_remote_module_non_sriptable.py
[2022-07-05 17:12:39,099][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 17:12:39,103][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 17:12:39,113][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 17:12:39,114][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 17:12:53,878][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-05 17:15:37,005][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpohdc2i_q
[2022-07-05 17:15:37,006][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpohdc2i_q/_remote_module_non_sriptable.py
[2022-07-05 17:15:38,633][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-05 17:15:38,636][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-05 17:15:38,642][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-05 17:15:38,642][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-05 17:15:53,033][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-06 13:50:19,300][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpj6hd8ffi
[2022-07-06 13:50:19,300][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpj6hd8ffi/_remote_module_non_sriptable.py
[2022-07-06 13:50:20,810][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-06 13:50:20,814][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-06 13:50:25,756][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-06 13:50:25,756][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
[2022-07-06 13:50:41,388][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-06 13:55:24,246][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-07-06 13:57:58,475][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpygb9wtph
[2022-07-06 13:57:58,475][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpygb9wtph/_remote_module_non_sriptable.py
[2022-07-06 13:58:00,023][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-06 13:58:00,026][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-06 13:58:00,032][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-06 13:58:00,032][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-06 13:58:10,786][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-06 14:15:30,976][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpnchw82fz
[2022-07-06 14:15:30,976][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpnchw82fz/_remote_module_non_sriptable.py
[2022-07-06 14:15:32,607][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-06 14:15:32,610][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-06 14:15:32,621][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-06 14:15:32,621][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-06 14:17:08,969][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpvclhawwx
[2022-07-06 14:17:08,970][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpvclhawwx/_remote_module_non_sriptable.py
[2022-07-06 14:17:10,755][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-06 14:17:10,759][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-06 14:17:10,770][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-06 14:17:10,770][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-06 14:18:20,927][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-06 14:35:58,280][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpgtp3lakb
[2022-07-06 14:35:58,280][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpgtp3lakb/_remote_module_non_sriptable.py
[2022-07-06 14:35:59,853][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-06 14:35:59,856][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-06 14:35:59,863][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-06 14:35:59,863][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-06 14:36:10,418][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-06 15:11:49,892][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-07-06 15:46:28,880][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmphc62o4o4
[2022-07-06 15:46:28,881][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmphc62o4o4/_remote_module_non_sriptable.py
[2022-07-06 15:46:30,737][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-06 15:46:30,741][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-06 15:46:30,752][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-06 15:46:30,753][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-06 15:57:42,266][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpvno0agqn
[2022-07-06 15:57:42,267][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpvno0agqn/_remote_module_non_sriptable.py
[2022-07-06 15:57:43,881][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-06 15:57:43,884][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-06 15:57:43,895][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-06 15:57:43,896][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-06 16:08:25,807][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpa6p1r4m3
[2022-07-06 16:08:25,807][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpa6p1r4m3/_remote_module_non_sriptable.py
[2022-07-06 16:08:27,484][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-06 16:08:27,490][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-06 16:08:27,498][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-06 16:08:27,499][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-06 16:08:45,452][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-06 16:09:44,817][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-07-06 16:12:13,127][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpkzxtax19
[2022-07-06 16:12:13,128][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpkzxtax19/_remote_module_non_sriptable.py
[2022-07-06 16:12:15,037][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-06 16:12:15,041][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-06 16:12:15,052][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-06 16:12:15,052][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-06 16:13:40,945][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmppuo_zno5
[2022-07-06 16:13:40,945][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmppuo_zno5/_remote_module_non_sriptable.py
[2022-07-06 16:13:42,560][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-06 16:13:42,574][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-06 16:13:42,580][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-06 16:13:42,581][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-06 16:13:59,586][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-06 16:24:30,152][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-07-06 16:27:23,537][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpg8hpupan
[2022-07-06 16:27:23,538][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpg8hpupan/_remote_module_non_sriptable.py
[2022-07-06 16:27:25,223][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-06 16:27:25,227][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-06 16:27:25,236][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-06 16:27:25,237][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-06 16:37:06,134][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpoeaui3q2
[2022-07-06 16:37:06,134][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpoeaui3q2/_remote_module_non_sriptable.py
[2022-07-06 16:37:07,754][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-06 16:37:07,758][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-06 16:37:07,763][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-06 16:37:07,763][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-06 16:37:18,898][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-06 17:06:32,425][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-07-06 17:54:44,260][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpa80icsip
[2022-07-06 17:54:44,261][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpa80icsip/_remote_module_non_sriptable.py
[2022-07-06 17:54:45,923][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-06 17:54:45,927][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-06 17:54:45,937][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-06 17:54:45,937][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-06 17:58:01,760][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpo4j0rqqm
[2022-07-06 17:58:01,760][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpo4j0rqqm/_remote_module_non_sriptable.py
[2022-07-06 17:58:03,579][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-06 17:58:03,583][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-06 17:58:03,593][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-06 17:58:03,593][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 14:32:02,701][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpqv2ozjlq
[2022-07-07 14:32:02,702][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpqv2ozjlq/_remote_module_non_sriptable.py
[2022-07-07 14:32:04,960][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 14:32:04,964][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 14:32:04,975][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 14:32:04,976][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 14:32:18,029][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-07 15:14:13,982][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpdfucfc3w
[2022-07-07 15:14:13,983][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpdfucfc3w/_remote_module_non_sriptable.py
[2022-07-07 15:14:16,180][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 15:14:16,185][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 15:14:16,196][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 15:14:16,196][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 15:14:28,716][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-07 15:30:58,866][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpjgkuhsam
[2022-07-07 15:30:58,866][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpjgkuhsam/_remote_module_non_sriptable.py
[2022-07-07 15:31:00,479][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 15:31:00,480][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 15:31:00,485][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 15:31:00,485][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 15:31:17,298][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-07 15:33:11,003][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp728yomt4
[2022-07-07 15:33:11,004][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp728yomt4/_remote_module_non_sriptable.py
[2022-07-07 15:33:12,572][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 15:33:12,574][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 15:33:12,580][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 15:33:12,580][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 15:33:28,205][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-07 16:45:38,231][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-07-07 18:48:05,951][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpitasmme2
[2022-07-07 18:48:05,952][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpitasmme2/_remote_module_non_sriptable.py
[2022-07-07 18:48:07,561][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 18:48:07,565][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 18:48:07,576][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 18:48:07,576][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 19:16:24,781][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmphprr_k51
[2022-07-07 19:16:24,781][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmphprr_k51/_remote_module_non_sriptable.py
[2022-07-07 19:16:26,422][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 19:16:26,426][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 19:16:26,436][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 19:16:26,436][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 19:18:22,001][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpdaajl3hh
[2022-07-07 19:18:22,001][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpdaajl3hh/_remote_module_non_sriptable.py
[2022-07-07 19:18:23,623][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 19:18:23,627][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 19:18:23,637][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 19:18:23,637][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 19:19:42,112][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp3ee04c3n
[2022-07-07 19:19:42,112][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp3ee04c3n/_remote_module_non_sriptable.py
[2022-07-07 19:19:43,659][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 19:19:43,663][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 19:19:43,673][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 19:19:43,673][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 19:29:00,600][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpj9fojkrq
[2022-07-07 19:29:00,600][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpj9fojkrq/_remote_module_non_sriptable.py
[2022-07-07 19:29:02,448][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 19:29:02,452][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 19:29:02,463][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 19:29:02,463][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 19:43:14,104][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpwrxc1t4x
[2022-07-07 19:43:14,104][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpwrxc1t4x/_remote_module_non_sriptable.py
[2022-07-07 19:43:15,972][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 19:43:15,976][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 19:43:15,986][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 19:43:15,986][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 19:53:15,995][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpd4uec6s8
[2022-07-07 19:53:15,996][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpd4uec6s8/_remote_module_non_sriptable.py
[2022-07-07 19:53:17,875][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 19:53:17,879][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 19:53:17,890][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 19:53:17,891][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 20:03:14,209][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpara_937x
[2022-07-07 20:03:14,210][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpara_937x/_remote_module_non_sriptable.py
[2022-07-07 20:03:15,926][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 20:03:15,930][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 20:03:15,941][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 20:03:15,941][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 20:11:01,816][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpjpn_19u6
[2022-07-07 20:11:01,816][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpjpn_19u6/_remote_module_non_sriptable.py
[2022-07-07 20:11:03,538][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 20:11:03,543][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 20:11:03,560][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 20:11:03,561][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 20:19:45,767][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpy4xfy475
[2022-07-07 20:19:45,767][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpy4xfy475/_remote_module_non_sriptable.py
[2022-07-07 20:19:47,478][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 20:19:47,480][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 20:19:47,485][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 20:19:47,485][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 20:21:48,387][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp4kw0vcho
[2022-07-07 20:21:48,388][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp4kw0vcho/_remote_module_non_sriptable.py
[2022-07-07 20:21:50,199][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 20:21:50,210][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 20:21:50,218][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 20:21:50,218][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 20:22:50,238][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp2ogbnjbr
[2022-07-07 20:22:50,239][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp2ogbnjbr/_remote_module_non_sriptable.py
[2022-07-07 20:22:52,009][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 20:22:52,010][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 20:22:52,017][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 20:22:52,018][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 20:25:33,664][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpsugeuiah
[2022-07-07 20:25:33,664][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpsugeuiah/_remote_module_non_sriptable.py
[2022-07-07 20:25:35,155][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 20:25:35,157][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 20:25:35,162][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 20:25:35,162][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 20:26:19,318][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp7mxb0oxl
[2022-07-07 20:26:19,319][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp7mxb0oxl/_remote_module_non_sriptable.py
[2022-07-07 20:26:20,901][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 20:26:20,902][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 20:26:20,908][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 20:26:20,908][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 20:26:30,382][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-07 20:31:02,751][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-07-07 20:34:00,941][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpty4k9o9o
[2022-07-07 20:34:00,941][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpty4k9o9o/_remote_module_non_sriptable.py
[2022-07-07 20:34:02,945][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 20:34:02,949][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 20:34:02,959][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 20:34:02,959][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 20:38:58,774][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp8gjdle7d
[2022-07-07 20:38:58,775][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp8gjdle7d/_remote_module_non_sriptable.py
[2022-07-07 20:39:00,415][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 20:39:00,416][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 20:39:00,422][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 20:39:00,422][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 20:39:09,977][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-07 20:43:58,751][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-07-07 20:45:15,425][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp_09anjm5
[2022-07-07 20:45:15,426][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp_09anjm5/_remote_module_non_sriptable.py
[2022-07-07 20:45:17,650][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 20:45:17,654][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 20:45:17,665][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 20:45:17,665][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 22:39:17,899][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpvi68y2s8
[2022-07-07 22:39:17,900][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpvi68y2s8/_remote_module_non_sriptable.py
[2022-07-07 22:39:19,531][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 22:39:19,533][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 22:39:19,538][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 22:39:19,539][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 22:41:21,513][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp43krzz0y
[2022-07-07 22:41:21,514][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp43krzz0y/_remote_module_non_sriptable.py
[2022-07-07 22:41:23,361][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 22:41:23,365][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 22:41:23,375][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 22:41:23,375][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 22:44:31,685][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp1aklujtu
[2022-07-07 22:44:31,687][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp1aklujtu/_remote_module_non_sriptable.py
[2022-07-07 22:44:35,085][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 22:44:35,092][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 22:44:35,107][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 22:44:35,107][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 22:46:36,304][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpjcbuebqc
[2022-07-07 22:46:36,304][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpjcbuebqc/_remote_module_non_sriptable.py
[2022-07-07 22:46:37,944][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 22:46:37,946][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 22:46:37,951][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 22:46:37,951][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 22:46:49,036][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-07 22:52:46,914][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-07-07 22:59:42,185][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpfjoc3byd
[2022-07-07 22:59:42,186][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpfjoc3byd/_remote_module_non_sriptable.py
[2022-07-07 22:59:44,679][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 22:59:44,683][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 22:59:44,694][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 22:59:44,695][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 23:00:11,042][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-07 23:06:26,856][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpp5cq4wjt
[2022-07-07 23:06:26,857][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpp5cq4wjt/_remote_module_non_sriptable.py
[2022-07-07 23:06:28,714][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 23:06:28,718][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 23:06:28,729][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 23:06:28,729][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 23:08:48,439][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpqrpocktk
[2022-07-07 23:08:48,439][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpqrpocktk/_remote_module_non_sriptable.py
[2022-07-07 23:08:51,852][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 23:08:51,856][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 23:08:51,866][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 23:08:51,867][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-07 23:12:37,627][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmpaxiwvzqn
[2022-07-07 23:12:37,627][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmpaxiwvzqn/_remote_module_non_sriptable.py
[2022-07-07 23:12:39,753][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-07 23:12:39,755][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-07 23:12:39,760][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-07 23:12:39,760][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-08 13:43:31,340][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp3kz3rk9_
[2022-07-08 13:43:31,341][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp3kz3rk9_/_remote_module_non_sriptable.py
[2022-07-08 13:43:32,966][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-08 13:43:32,968][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-08 13:43:32,976][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-08 13:43:32,977][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-08 13:46:25,515][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp58d7s3m4
[2022-07-08 13:46:25,515][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp58d7s3m4/_remote_module_non_sriptable.py
[2022-07-08 13:46:27,194][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-08 13:46:27,197][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-08 13:46:27,206][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-08 13:46:27,206][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-08 13:46:44,559][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

[2022-07-08 13:54:01,500][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")

[2022-07-08 13:57:25,449][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmprrhbjdvq
[2022-07-08 13:57:25,449][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmprrhbjdvq/_remote_module_non_sriptable.py
[2022-07-08 13:57:26,991][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  f"Passing `Trainer(accelerator={self.distributed_backend!r})` has been deprecated"

[2022-07-08 13:57:26,994][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.
  f"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will "

[2022-07-08 13:57:27,001][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-08 13:57:27,001][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
[2022-07-08 13:57:42,375][py.warnings][WARNING] - /raid/rishubh/conda/aconda3/envs/stylegan/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
                
  warnings.warn(*args, **kwargs)

